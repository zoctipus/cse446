
    \section{}
    
    \indent\indent a. 
    False.
    
    Deep neural network's loss function are non convex. Gradient descent provide some local minima, but no guarantee the local minima will be global minima
    
    \vspace{1em}
    b. False.
    
    There is not preference what is the best weight, a randomly initialized value may work as good as zero initialization or even better. Though initialize to 0 is better than initialized to some highly variant, exploding values.
    
    \vspace{1em}
    c. True. 
    
    Non-linear activation functions in a neural network's hidden layers enable the network to learn non-linear decision boundaries.
    
    \vspace{1em}
    d. False
    
    The time complexities of both forward and backward passes in a neural network are comparable. For each layer, computations involve matrix multiplications, resulting in \(O(nm)\) complexity, where \(n\) and \(m\) are matrix dimensions. Additional gradient calculations in the backward pass involves in the same multiplication, and do not significantly alter this complexity.
    
    
    \vspace{1em}
    e. False. 
    
    Neural Networks, though versatile, are not always the best choice; their suitability varies with factors like data complexity, computational resources, and interpret-ability needs, where simpler models may be preferable.
    