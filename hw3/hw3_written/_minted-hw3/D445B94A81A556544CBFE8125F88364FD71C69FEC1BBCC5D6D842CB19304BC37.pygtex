\begin{Verbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{torch}
\PYG{k+kn}{from} \PYG{n+nn}{torch} \PYG{k+kn}{import} \PYG{n}{nn}

\PYG{k+kn}{from} \PYG{n+nn}{utils} \PYG{k+kn}{import} \PYG{n}{problem}


\PYG{k}{class} \PYG{n+nc}{CrossEntropyLossLayer}\PYG{p}{(}\PYG{n}{nn}\PYG{o}{.}\PYG{n}{Module}\PYG{p}{):}
    \PYG{n+nd}{@problem}\PYG{o}{.}\PYG{n}{tag}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}hw3\PYGZhy{}A\PYGZdq{}}\PYG{p}{)}
    \PYG{k}{def} \PYG{n+nf}{forward}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{y\PYGZus{}pred}\PYG{p}{:} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{Tensor}\PYG{p}{,} \PYG{n}{y\PYGZus{}true}\PYG{p}{:} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{Tensor}\PYG{p}{)} \PYG{o}{\PYGZhy{}\PYGZgt{}} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{Tensor}\PYG{p}{:}
\PYG{+w}{        }\PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}Calculate Crossentropy loss based on (normalized) predictions and true categories/classes.}

\PYG{l+s+sd}{        For a single example (x is vector of predictions, y is correct class):}

\PYG{l+s+sd}{        cross\PYGZus{}entropy(x, y) = \PYGZhy{}log(x[y])}

\PYG{l+s+sd}{        Args:}
\PYG{l+s+sd}{            y\PYGZus{}pred (torch.Tensor): More specifically a torch.FloatTensor, with shape (n, c).}
\PYG{l+s+sd}{                Predictions of classes. Each row is normalized so that L\PYGZhy{}1 norm is 1 (Each row is proper probability vector).}
\PYG{l+s+sd}{                Input data.}
\PYG{l+s+sd}{            y\PYGZus{}true (torch.Tensor): More specifically a torch.LongTensor, with shape (n,).}
\PYG{l+s+sd}{                Each element is an integer in range [0, c).}
\PYG{l+s+sd}{                Input data.}

\PYG{l+s+sd}{        Returns:}
\PYG{l+s+sd}{            torch.Tensor: More specifically a SINGLE VALUE torch.FloatTensor (i.e. with shape (1,)).}
\PYG{l+s+sd}{                Should be a mean over all examples.}
\PYG{l+s+sd}{                Result.}

\PYG{l+s+sd}{        Note:}
\PYG{l+s+sd}{            \PYGZhy{} YOU ARE NOT ALLOWED to use torch.nn.CrossEntropyLoss / torch.nn.NLLLoss (or their functional counterparts) in this class}
\PYG{l+s+sd}{            \PYGZhy{} Make use of pytorch documentation: https://pytorch.org/docs/stable/index.html}
\PYG{l+s+sd}{            \PYGZhy{} Not that this is different from torch.nn.CrossEntropyLoss, as it doesn\PYGZsq{}t perform softmax, but anticipates the result to already be normalized.}
\PYG{l+s+sd}{        \PYGZdq{}\PYGZdq{}\PYGZdq{}}
        \PYG{n}{individual\PYGZus{}losses} \PYG{o}{=} \PYG{o}{\PYGZhy{}}\PYG{n}{torch}\PYG{o}{.}\PYG{n}{log}\PYG{p}{(}\PYG{n}{y\PYGZus{}pred}\PYG{p}{[}\PYG{n}{torch}\PYG{o}{.}\PYG{n}{arange}\PYG{p}{(}\PYG{n}{y\PYGZus{}pred}\PYG{o}{.}\PYG{n}{size}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{)),} \PYG{n}{y\PYGZus{}true}\PYG{p}{])}
        \PYG{k}{return} \PYG{n}{individual\PYGZus{}losses}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{()}
\end{Verbatim}
