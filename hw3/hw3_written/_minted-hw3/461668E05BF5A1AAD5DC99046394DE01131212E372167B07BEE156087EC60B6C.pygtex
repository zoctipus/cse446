\begin{Verbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{torch}

\PYG{k+kn}{from} \PYG{n+nn}{utils} \PYG{k+kn}{import} \PYG{n}{problem}


\PYG{k}{class} \PYG{n+nc}{SGDOptimizer}\PYG{p}{(}\PYG{n}{torch}\PYG{o}{.}\PYG{n}{optim}\PYG{o}{.}\PYG{n}{Optimizer}\PYG{p}{):}
    \PYG{n+nd}{@problem}\PYG{o}{.}\PYG{n}{tag}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}hw3\PYGZhy{}A\PYGZdq{}}\PYG{p}{,} \PYG{n}{start\PYGZus{}line}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}
    \PYG{k}{def} \PYG{n+nf+fm}{\PYGZus{}\PYGZus{}init\PYGZus{}\PYGZus{}}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{params}\PYG{p}{,} \PYG{n}{lr}\PYG{p}{:} \PYG{n+nb}{float}\PYG{p}{)} \PYG{o}{\PYGZhy{}\PYGZgt{}} \PYG{k+kc}{None}\PYG{p}{:}
\PYG{+w}{        }\PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}Constructor for Stochastic Gradient Descent (SGD) Optimizer.}

\PYG{l+s+sd}{        Provided code contains call to super class, which will initialize paramaters properly (see step function docs).}
\PYG{l+s+sd}{        This class will only update the parameters provided to it, based on their (already calculated) gradients.}

\PYG{l+s+sd}{        Args:}
\PYG{l+s+sd}{            params: Parameters to update each step. You don\PYGZsq{}t need to do anything with them.}
\PYG{l+s+sd}{                They are properly initialize through the super call.}
\PYG{l+s+sd}{            lr (float): Learning Rate of the gradient descent.}

\PYG{l+s+sd}{        Note:}
\PYG{l+s+sd}{            \PYGZhy{} YOU ARE NOT ALLOWED to use torch.optim.SGD in this class}
\PYG{l+s+sd}{            \PYGZhy{} While you are not allowed to use the class above, it might be extremely beneficial to look at it\PYGZsq{}s code when implementing step function.}
\PYG{l+s+sd}{            \PYGZhy{} Make use of pytorch documentation: https://pytorch.org/docs/stable/index.html}
\PYG{l+s+sd}{        \PYGZdq{}\PYGZdq{}\PYGZdq{}}
        \PYG{n+nb}{super}\PYG{p}{()}\PYG{o}{.}\PYG{n+nf+fm}{\PYGZus{}\PYGZus{}init\PYGZus{}\PYGZus{}}\PYG{p}{(}\PYG{n}{params}\PYG{p}{,} \PYG{p}{\PYGZob{}}\PYG{l+s+s2}{\PYGZdq{}lr\PYGZdq{}}\PYG{p}{:} \PYG{n}{lr}\PYG{p}{\PYGZcb{})}

    \PYG{n+nd}{@problem}\PYG{o}{.}\PYG{n}{tag}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}hw3\PYGZhy{}A\PYGZdq{}}\PYG{p}{)}
    \PYG{k}{def} \PYG{n+nf}{step}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{closure}\PYG{o}{=}\PYG{k+kc}{None}\PYG{p}{):}  \PYG{c+c1}{\PYGZsh{} noqa: E251}
\PYG{+w}{        }\PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}
\PYG{l+s+sd}{        Performs a step of gradient descent. You should loop through each parameter, and update it\PYGZsq{}s value based on its gradient, value and learning rate.}

\PYG{l+s+sd}{        Args:}
\PYG{l+s+sd}{            closure (optional): Ignore this. We will not use in this class, but it is required for subclassing Optimizer.}
\PYG{l+s+sd}{                Defaults to None.}

\PYG{l+s+sd}{        Hint:}
\PYG{l+s+sd}{            \PYGZhy{} Superclass stores parameters in self.param\PYGZus{}groups (you will have to discover in what format).}
\PYG{l+s+sd}{        \PYGZdq{}\PYGZdq{}\PYGZdq{}}
        \PYG{k}{for} \PYG{n}{group} \PYG{o+ow}{in} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{param\PYGZus{}groups}\PYG{p}{:}
            \PYG{k}{for} \PYG{n}{p} \PYG{o+ow}{in} \PYG{n}{group}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}params\PYGZsq{}}\PYG{p}{]:}
                \PYG{n}{p}\PYG{o}{.}\PYG{n}{data} \PYG{o}{\PYGZhy{}=} \PYG{n}{group}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}lr\PYGZsq{}}\PYG{p}{]} \PYG{o}{*} \PYG{n}{p}\PYG{o}{.}\PYG{n}{grad}\PYG{o}{.}\PYG{n}{data}
\end{Verbatim}
